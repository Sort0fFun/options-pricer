{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility Forecasting Model for CME Globex Futures\n",
    "\n",
    "**Tested and verified to work with Databento GLBX-MDP3 trade data.**\n",
    "\n",
    "This notebook builds an enhanced machine learning model to predict future realized volatility.\n",
    "\n",
    "**Strategy:** Predict whether volatility over the next hour will be higher or lower than current volatility.\n",
    "\n",
    "## What's New in:\n",
    "1. ✅ **Advanced Volatility Features**: Yang-Zhang, Rogers-Satchell estimators\n",
    "2. ✅ **Fixed Backtest**: Realistic transaction costs and position sizing\n",
    "3. ✅ **Time Series CV with Gaps**: Proper cross-validation preventing lookahead bias\n",
    "4. ✅ **Probability Calibration**: Calibrated prediction probabilities\n",
    "5. ✅ **LightGBM Model**: Additional fast gradient boosting model\n",
    "6. ✅ **SHAP Analysis**: Model interpretability with SHAP values\n",
    "7. ✅ **Regime Analysis**: Performance breakdown by market conditions\n",
    "8. ✅ **Feature Selection**: Recursive feature elimination\n",
    "9. ✅ **Confidence Filtering**: Only trade high-confidence predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Google Colab\n",
    "!pip install -q zstandard pandas numpy scikit-learn xgboost lightgbm matplotlib seaborn joblib shap\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print('✓ Google Drive mounted successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Google Colab doesn't need the OpenMP fix (it's handled automatically)\n",
    "import zstandard as zstd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, brier_score_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import RFECV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "# Try to import SHAP (optional for interpretability)\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print('✓ SHAP available for model interpretability')\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print('⚠ SHAP not available (pip install shap)')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "print('✓ All imports successful!')\n",
    "print(f'✓ XGBoost version: {xgb.__version__}')\n",
    "print(f'✓ LightGBM version: {lgb.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these as needed\n",
    "# =============================================================================\n",
    "\n",
    "# Google Drive path - UPDATE THIS to match your folder structure\n",
    "# Example: If your data is in \"My Drive/options-pricer/data\", use:\n",
    "DATA_DIR = '/content/drive/MyDrive/options-data'\n",
    "\n",
    "# Alternative paths (uncomment the one that matches your setup):\n",
    "# DATA_DIR = '/content/drive/MyDrive/data'\n",
    "# DATA_DIR = '/content/drive/Shareddrives/YourSharedDrive/data'\n",
    "\n",
    "SYMBOL = 'NQZ5'\n",
    "BAR_FREQ = '5min'\n",
    "HORIZON = 12  # 12 bars = 1 hour\n",
    "TRAIN_RATIO = 0.7\n",
    "TRANSACTION_COST = 0.0005  # 5 bps per trade (10 bps round-trip)\n",
    "CONFIDENCE_THRESHOLD = 0.6  # Only trade predictions with >60% confidence\n",
    "GAP_SIZE = 24  # 2-hour gap in time series CV to prevent data leakage\n",
    "\n",
    "# Verify the data directory exists\n",
    "if os.path.exists(DATA_DIR):\n",
    "    files = list(Path(DATA_DIR).glob('glbx-mdp3-*.zst'))\n",
    "    print(f'✓ Data directory found: {DATA_DIR}')\n",
    "    print(f'✓ Found {len(files)} data files')\n",
    "else:\n",
    "    print(f'⚠ Data directory not found: {DATA_DIR}')\n",
    "    print('  Please update DATA_DIR to match your Google Drive folder structure')\n",
    "    print('  Your Google Drive is mounted at: /content/drive/MyDrive/')\n",
    "\n",
    "print(f'\\nConfiguration:')\n",
    "print(f'  Data Directory: {DATA_DIR}')\n",
    "print(f'  Symbol: {SYMBOL}')\n",
    "print(f'  Bar Frequency: {BAR_FREQ}')\n",
    "print(f'  Prediction Horizon: {HORIZON} bars ({HORIZON * 5} minutes)')\n",
    "print(f'  Confidence Threshold: {CONFIDENCE_THRESHOLD*100:.0f}%')\n",
    "print(f'  CV Gap Size: {GAP_SIZE} bars ({GAP_SIZE * 5} minutes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_zst_file(filepath):\n    \"\"\"Load a single zstd compressed CSV file.\n    \n    Args:\n        filepath: Path to the .zst compressed CSV file\n        \n    Returns:\n        pd.DataFrame: Loaded data from the compressed file\n    \"\"\"\n    try:\n        with open(filepath, 'rb') as f:\n            dctx = zstd.ZstdDecompressor()\n            reader = dctx.stream_reader(f)\n            chunks = []\n            while True:\n                chunk = reader.read(1024 * 1024 * 10)  # 10MB chunks\n                if not chunk:\n                    break\n                chunks.append(chunk)\n            data = b''.join(chunks)\n        return pd.read_csv(io.BytesIO(data))\n    except Exception as e:\n        print(f'Error decompressing {filepath}: {e}')\n        raise\n\ndef load_all_data(data_dir, symbol):\n    \"\"\"Load all Databento GLBX-MDP3 trade data files and filter by symbol.\n    Memory-optimized version that processes in chunks.\n    \n    Args:\n        data_dir: Directory containing .zst compressed trade files\n        symbol: Futures symbol to filter (e.g., 'NQZ5')\n        \n    Returns:\n        pd.DataFrame: Combined trade data for the specified symbol\n    \"\"\"\n    data_path = Path(data_dir)\n    \n    # Debug: Show what we're looking for\n    print(f'Searching in: {data_path}')\n    print(f'Path exists: {data_path.exists()}')\n    \n    if data_path.exists():\n        print(f'Directory contents: {list(data_path.iterdir())[:5]}...')  # Show first 5 items\n    \n    # Try multiple glob patterns to find the files\n    patterns = [\n        'glbx-mdp3-*.trades.csv.zst',  # Full pattern with .trades.csv.zst\n        'glbx-mdp3-*.csv.zst',          # Pattern with .csv.zst\n        'glbx-mdp3-*.zst',              # Simple .zst pattern\n        '*.zst'                          # Any zst file\n    ]\n    \n    files = []\n    for pattern in patterns:\n        files = sorted(data_path.glob(pattern))\n        if files:\n            print(f'✓ Found {len(files)} files matching pattern: {pattern}')\n            break\n        else:\n            print(f'  No files matching: {pattern}')\n    \n    if not files:\n        raise FileNotFoundError(\n            f'No compressed data files found in {data_dir}\\n'\n            f'Expected files like: glbx-mdp3-20250928.trades.csv.zst\\n'\n            f'Directory exists: {data_path.exists()}'\n        )\n    \n    print(f'\\nLoading {len(files)} data files for symbol: {symbol}')\n    \n    # OPTIMIZATION: Process files in batches and keep only essential columns\n    all_dfs = []\n    batch_size = 10  # Combine every 10 files\n    current_batch = []\n    \n    # Essential columns only - reduce memory footprint\n    essential_cols = ['ts_event', 'price', 'size', 'side']\n    \n    for i, f in enumerate(files):\n        try:\n            print(f'  [{i+1}/{len(files)}] Loading {f.name}...', end=' ')\n            df = load_zst_file(str(f))\n            \n            if len(df) == 0:\n                print('Empty file')\n                continue\n                \n            if 'symbol' not in df.columns:\n                print(f'No symbol column. Columns: {df.columns.tolist()}')\n                continue\n            \n            # Filter by symbol and select only essential columns immediately\n            df_filtered = df[df['symbol'] == symbol][essential_cols].copy()\n            \n            if len(df_filtered) > 0:\n                # Convert timestamp immediately to reduce sort time later\n                df_filtered['ts_event'] = pd.to_datetime(df_filtered['ts_event'])\n                current_batch.append(df_filtered)\n                print(f'{len(df_filtered):,} trades')\n            else:\n                # Show available symbols for debugging\n                available = df['symbol'].unique()[:5]\n                print(f'0 trades (available symbols: {available})')\n                \n            # OPTIMIZATION: Combine and sort batches periodically\n            if len(current_batch) >= batch_size:\n                print(f'  → Combining batch of {len(current_batch)} dataframes...')\n                batch_combined = pd.concat(current_batch, ignore_index=True)\n                # Sort each batch - much faster than sorting the entire dataset at once\n                batch_combined = batch_combined.sort_values('ts_event').reset_index(drop=True)\n                all_dfs.append(batch_combined)\n                current_batch = []\n                print(f'  → Batch sorted ({len(batch_combined):,} rows)')\n                \n        except Exception as e:\n            print(f'Error: {e}')\n    \n    # Add remaining batch\n    if current_batch:\n        print(f'  → Combining final batch of {len(current_batch)} dataframes...')\n        batch_combined = pd.concat(current_batch, ignore_index=True)\n        batch_combined = batch_combined.sort_values('ts_event').reset_index(drop=True)\n        all_dfs.append(batch_combined)\n        print(f'  → Final batch sorted ({len(batch_combined):,} rows)')\n    \n    if not all_dfs:\n        raise ValueError(\n            f'No data found for symbol {symbol}\\n'\n            f'Loaded {len(files)} files but none contained {symbol}'\n        )\n    \n    print(f'\\nMerging {len(all_dfs)} pre-sorted batches (this is much faster)...')\n    # Since batches are already sorted and files are in chronological order,\n    # we can just concatenate them - they should already be mostly in order\n    combined = pd.concat(all_dfs, ignore_index=True)\n    \n    # Final sort - should be much faster since data is already mostly sorted\n    print('Final sort (data already mostly sorted)...')\n    combined = combined.sort_values('ts_event').reset_index(drop=True)\n    \n    print(f'✓ Loading complete!')\n    return combined\n\nprint('='*60)\nprint('LOADING DATA')\nprint('='*60)\n\nraw_data = load_all_data(DATA_DIR, SYMBOL)\n\nprint(f'\\n✓ Total trades loaded: {len(raw_data):,}')\nprint(f'✓ Date range: {raw_data[\"ts_event\"].min()} to {raw_data[\"ts_event\"].max()}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create OHLCV Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bars(df, freq='5min'):\n",
    "    \"\"\"Aggregate tick data into OHLCV bars\"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.set_index('ts_event')\n",
    "    df['signed_volume'] = df['size'] * df['side'].map({'B': 1, 'A': -1, 'N': 0})\n",
    "    \n",
    "    bars = df.groupby(pd.Grouper(freq=freq)).agg({\n",
    "        'price': ['first', 'max', 'min', 'last', 'std', 'count'],\n",
    "        'size': ['sum', 'mean', 'max', 'std'],\n",
    "        'signed_volume': 'sum'\n",
    "    })\n",
    "    \n",
    "    bars.columns = ['open', 'high', 'low', 'close', 'price_std', 'tick_count',\n",
    "                    'volume', 'avg_trade_size', 'max_trade_size', 'trade_size_std',\n",
    "                    'order_flow_imbalance']\n",
    "    \n",
    "    bars = bars[bars['volume'] > 0].copy()\n",
    "    bars['return'] = bars['close'].pct_change()\n",
    "    bars['log_return'] = np.log(bars['close'] / bars['close'].shift(1))\n",
    "    bars['range'] = (bars['high'] - bars['low']) / bars['close']\n",
    "    bars['body'] = abs(bars['close'] - bars['open']) / bars['close']\n",
    "    bars['upper_wick'] = (bars['high'] - bars[['open', 'close']].max(axis=1)) / bars['close']\n",
    "    bars['lower_wick'] = (bars[['open', 'close']].min(axis=1) - bars['low']) / bars['close']\n",
    "    bars['ofi_normalized'] = bars['order_flow_imbalance'] / bars['volume']\n",
    "    \n",
    "    return bars\n",
    "\n",
    "bars = create_bars(raw_data, BAR_FREQ)\n",
    "print(f'Created {len(bars):,} bars at {BAR_FREQ} frequency')\n",
    "print(f'Date range: {bars.index.min()} to {bars.index.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhanced Feature Engineering\n",
    "\n",
    "### 5.1 Advanced Volatility Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_realized_volatility(returns, window):\n",
    "    return returns.rolling(window=window).std()\n",
    "\n",
    "def calculate_parkinson_volatility(high, low, window):\n",
    "    \"\"\"Parkinson volatility estimator\"\"\"\n",
    "    log_hl = np.log(high / low)\n",
    "    return np.sqrt((log_hl ** 2).rolling(window=window).mean() / (4 * np.log(2)))\n",
    "\n",
    "def calculate_garman_klass_volatility(open_, high, low, close, window):\n",
    "    \"\"\"Garman-Klass volatility estimator\"\"\"\n",
    "    log_hl = np.log(high / low) ** 2\n",
    "    log_co = np.log(close / open_) ** 2\n",
    "    return np.sqrt((0.5 * log_hl - (2 * np.log(2) - 1) * log_co).rolling(window=window).mean())\n",
    "\n",
    "def calculate_rogers_satchell_volatility(open_, high, low, close, window):\n",
    "    \"\"\"Rogers-Satchell volatility estimator (drift-independent)\"\"\"\n",
    "    log_ho = np.log(high / open_)\n",
    "    log_hc = np.log(high / close)\n",
    "    log_lo = np.log(low / open_)\n",
    "    log_lc = np.log(low / close)\n",
    "    return np.sqrt((log_ho * log_hc + log_lo * log_lc).rolling(window=window).mean())\n",
    "\n",
    "def calculate_yang_zhang_volatility(open_, high, low, close, window):\n",
    "    \"\"\"Yang-Zhang volatility estimator (handles overnight gaps)\"\"\"\n",
    "    # Overnight volatility\n",
    "    log_co = np.log(open_ / close.shift(1))\n",
    "    overnight_vol = log_co.rolling(window=window).var()\n",
    "    \n",
    "    # Open to close volatility (Rogers-Satchell)\n",
    "    rs_vol = calculate_rogers_satchell_volatility(open_, high, low, close, window) ** 2\n",
    "    \n",
    "    # Close to close volatility\n",
    "    log_cc = np.log(close / close.shift(1))\n",
    "    close_vol = log_cc.rolling(window=window).var()\n",
    "    \n",
    "    # Yang-Zhang combination\n",
    "    k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
    "    yang_zhang = overnight_vol + k * close_vol + (1 - k) * rs_vol\n",
    "    \n",
    "    return np.sqrt(yang_zhang)\n",
    "\n",
    "print('✓ Advanced volatility estimators defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create All Features (Including New Ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_features(bars):\n",
    "    \"\"\"Create enhanced feature set with advanced volatility estimators\"\"\"\n",
    "    df = bars.copy()\n",
    "    \n",
    "    print('Creating volatility features...')\n",
    "    # Basic realized volatility\n",
    "    for window in [6, 12, 24, 48, 96]:\n",
    "        df[f'rvol_{window}'] = calculate_realized_volatility(df['log_return'], window)\n",
    "    \n",
    "    # Parkinson volatility\n",
    "    for window in [6, 12, 24, 48]:\n",
    "        df[f'parkinson_{window}'] = calculate_parkinson_volatility(df['high'], df['low'], window)\n",
    "    \n",
    "    # Garman-Klass volatility\n",
    "    for window in [6, 12, 24, 48]:\n",
    "        df[f'gk_{window}'] = calculate_garman_klass_volatility(df['open'], df['high'], df['low'], df['close'], window)\n",
    "    \n",
    "    # NEW: Rogers-Satchell volatility\n",
    "    for window in [12, 24, 48]:\n",
    "        df[f'rs_{window}'] = calculate_rogers_satchell_volatility(df['open'], df['high'], df['low'], df['close'], window)\n",
    "    \n",
    "    # NEW: Yang-Zhang volatility\n",
    "    for window in [12, 24, 48]:\n",
    "        df[f'yz_{window}'] = calculate_yang_zhang_volatility(df['open'], df['high'], df['low'], df['close'], window)\n",
    "    \n",
    "    # Volatility dynamics\n",
    "    df['vol_of_vol'] = df['rvol_12'].rolling(12).std()\n",
    "    df['vol_ratio_12_48'] = df['rvol_12'] / df['rvol_48']\n",
    "    df['vol_ratio_6_24'] = df['rvol_6'] / df['rvol_24']\n",
    "    \n",
    "    # NEW: Volatility regime persistence\n",
    "    vol_percentile = df['rvol_24'].rolling(window=96).apply(lambda x: (x.iloc[-1] > x).sum() / len(x))\n",
    "    df['vol_percentile_96'] = vol_percentile\n",
    "    df['high_vol_regime'] = (vol_percentile > 0.75).astype(int)\n",
    "    df['low_vol_regime'] = (vol_percentile < 0.25).astype(int)\n",
    "    \n",
    "    print('Creating return features...')\n",
    "    # Return features\n",
    "    for lag in [1, 2, 3, 6, 12]:\n",
    "        df[f'return_lag_{lag}'] = df['log_return'].shift(lag)\n",
    "    \n",
    "    for window in [6, 12, 24]:\n",
    "        df[f'cum_return_{window}'] = df['log_return'].rolling(window).sum()\n",
    "    \n",
    "    df['return_momentum'] = df['cum_return_6'] - df['cum_return_12'].shift(6)\n",
    "    \n",
    "    for window in [6, 12, 24]:\n",
    "        df[f'abs_return_ma_{window}'] = df['log_return'].abs().rolling(window).mean()\n",
    "    \n",
    "    df['return_skew_24'] = df['log_return'].rolling(24).skew()\n",
    "    df['return_kurt_24'] = df['log_return'].rolling(24).kurt()\n",
    "    \n",
    "    print('Creating range features...')\n",
    "    # Range features\n",
    "    for window in [6, 12, 24]:\n",
    "        df[f'range_ma_{window}'] = df['range'].rolling(window).mean()\n",
    "    \n",
    "    df['range_vs_ma'] = df['range'] / df['range_ma_12']\n",
    "    \n",
    "    for window in [12, 24, 48]:\n",
    "        df[f'range_max_{window}'] = df['range'].rolling(window).max()\n",
    "    \n",
    "    print('Creating volume features...')\n",
    "    # Volume features\n",
    "    for window in [6, 12, 24, 48]:\n",
    "        df[f'volume_ma_{window}'] = df['volume'].rolling(window).mean()\n",
    "    \n",
    "    df['volume_ratio'] = df['volume'] / df['volume_ma_24']\n",
    "    df['volume_trend'] = df['volume_ma_6'] / df['volume_ma_24']\n",
    "    df['volume_std_24'] = df['volume'].rolling(24).std()\n",
    "    \n",
    "    print('Creating microstructure features...')\n",
    "    # Microstructure features\n",
    "    for window in [6, 12, 24]:\n",
    "        df[f'tick_count_ma_{window}'] = df['tick_count'].rolling(window).mean()\n",
    "    \n",
    "    df['tick_intensity_ratio'] = df['tick_count'] / df['tick_count_ma_24']\n",
    "    \n",
    "    for window in [6, 12, 24]:\n",
    "        df[f'avg_size_ma_{window}'] = df['avg_trade_size'].rolling(window).mean()\n",
    "    \n",
    "    for window in [6, 12, 24]:\n",
    "        df[f'ofi_ma_{window}'] = df['ofi_normalized'].rolling(window).mean()\n",
    "    \n",
    "    df['ofi_std_24'] = df['ofi_normalized'].rolling(24).std()\n",
    "    \n",
    "    for window in [6, 12, 24]:\n",
    "        df[f'ofi_cum_{window}'] = df['order_flow_imbalance'].rolling(window).sum()\n",
    "    \n",
    "    print('Creating time features...')\n",
    "    # Time features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dow'] = df.index.dayofweek\n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['dow'] / 5)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['dow'] / 5)\n",
    "    df['is_us_hours'] = ((df['hour'] >= 14) & (df['hour'] <= 20)).astype(int)\n",
    "    \n",
    "    print('Creating interaction features...')\n",
    "    # Interaction features\n",
    "    df['vol_x_volume'] = df['rvol_12'] * df['volume_ratio']\n",
    "    df['ofi_x_vol'] = df['ofi_normalized'].abs() * df['rvol_12']\n",
    "    \n",
    "    print('✓ Feature creation complete')\n",
    "    return df\n",
    "\n",
    "def create_target(bars, horizon=12):\n",
    "    \"\"\"Create target: will future vol be higher than current?\"\"\"\n",
    "    df = bars.copy()\n",
    "    future_rvol = df['log_return'].shift(-horizon).rolling(horizon).std()\n",
    "    current_rvol = df['rvol_12']\n",
    "    df['target_vol_up'] = (future_rvol > current_rvol).astype(int)\n",
    "    df['target_vol_level'] = future_rvol\n",
    "    df['target_vol_ratio'] = future_rvol / current_rvol\n",
    "    return df\n",
    "\n",
    "def get_feature_columns(df):\n",
    "    exclude = ['open', 'high', 'low', 'close', 'volume', 'log_return', 'return',\n",
    "               'target_vol_up', 'target_vol_level', 'target_vol_ratio',\n",
    "               'price_std', 'tick_count', 'avg_trade_size', 'max_trade_size',\n",
    "               'trade_size_std', 'order_flow_imbalance', 'range', 'body',\n",
    "               'upper_wick', 'lower_wick', 'ofi_normalized', 'hour', 'dow']\n",
    "    return [c for c in df.columns if c not in exclude]\n",
    "\n",
    "# Create features\n",
    "df_features = create_enhanced_features(bars)\n",
    "df_features = create_target(df_features, horizon=HORIZON)\n",
    "df_features = df_features.dropna()\n",
    "feature_columns = get_feature_columns(df_features)\n",
    "\n",
    "print(f'\\n✓ Total features: {len(feature_columns)}')\n",
    "print(f'✓ Total samples: {len(df_features)}')\n",
    "print(f'\\nNew features added:')\n",
    "new_features = [f for f in feature_columns if f.startswith(('rs_', 'yz_', 'vol_percentile', 'high_vol_regime', 'low_vol_regime'))]\n",
    "for f in new_features:\n",
    "    print(f'  - {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split with Proper Time Series Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "split_idx = int(len(df_features) * TRAIN_RATIO)\n",
    "train_df = df_features.iloc[:split_idx].copy()\n",
    "test_df = df_features.iloc[split_idx:].copy()\n",
    "\n",
    "print(f'Train: {len(train_df)} samples ({train_df.index[0]} to {train_df.index[-1]})')\n",
    "print(f'Test:  {len(test_df)} samples ({test_df.index[0]} to {test_df.index[-1]})')\n",
    "print(f'Target balance - Train: {train_df[\"target_vol_up\"].mean()*100:.1f}% up, Test: {test_df[\"target_vol_up\"].mean()*100:.1f}% up')\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_df[feature_columns].values\n",
    "y_train = train_df['target_vol_up'].values\n",
    "X_test = test_df[feature_columns].values\n",
    "y_test = test_df['target_vol_up'].values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'\\nX_train shape: {X_train_scaled.shape}')\n",
    "print(f'X_test shape: {X_test_scaled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', C=0.1),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=6, min_samples_leaf=20, \n",
    "                                            class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.05,\n",
    "                                  subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "                                  use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=100, max_depth=4, learning_rate=0.05,\n",
    "                                    subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "                                    class_weight='balanced', verbose=-1)\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f'Training {name}...', end=' ')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    trained_models[name] = model\n",
    "    print('Done!')\n",
    "\n",
    "print('\\n✓ All models trained successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation with Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "results = {}\n",
    "baseline_acc = max(y_test.mean(), 1 - y_test.mean())\n",
    "\n",
    "print(f'Baseline (majority class): {baseline_acc:.4f}')\n",
    "print('\\n' + '='*70)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, y_prob),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'brier': brier_score_loss(y_test, y_prob),\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_prob\n",
    "    }\n",
    "    \n",
    "    edge = results[name]['accuracy'] - baseline_acc\n",
    "    print(f'\\n{name}:')\n",
    "    print(f'  Accuracy:     {results[name][\"accuracy\"]:.4f} (edge: {edge*100:+.2f}%)')\n",
    "    print(f'  AUC:          {results[name][\"auc\"]:.4f}')\n",
    "    print(f'  Precision:    {results[name][\"precision\"]:.4f}')\n",
    "    print(f'  F1:           {results[name][\"f1\"]:.4f}')\n",
    "    print(f'  Brier Score:  {results[name][\"brier\"]:.4f} (lower is better)')\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(results, key=lambda x: results[x]['auc'])\n",
    "print(f'\\n✓ Best model: {best_model_name} (AUC: {results[best_model_name][\"auc\"]:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Probability Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the best model\n",
    "print(f'Calibrating {best_model_name}...')\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "# Use isotonic regression for calibration\n",
    "calibrated_model = CalibratedClassifierCV(best_model, method='isotonic', cv='prefit')\n",
    "calibrated_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get calibrated probabilities\n",
    "y_prob_calibrated = calibrated_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Compare calibration\n",
    "brier_before = brier_score_loss(y_test, results[best_model_name]['probabilities'])\n",
    "brier_after = brier_score_loss(y_test, y_prob_calibrated)\n",
    "\n",
    "print(f'\\nBrier Score before calibration: {brier_before:.4f}')\n",
    "print(f'Brier Score after calibration:  {brier_after:.4f}')\n",
    "print(f'Improvement: {(brier_before - brier_after)*100:.2f}%')\n",
    "\n",
    "# Update results with calibrated probabilities\n",
    "results[best_model_name + ' (Calibrated)'] = {\n",
    "    'accuracy': accuracy_score(y_test, (y_prob_calibrated > 0.5).astype(int)),\n",
    "    'auc': roc_auc_score(y_test, y_prob_calibrated),\n",
    "    'brier': brier_after,\n",
    "    'probabilities': y_prob_calibrated\n",
    "}\n",
    "\n",
    "trained_models[best_model_name + ' (Calibrated)'] = calibrated_model\n",
    "print('\\n✓ Calibration complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Time Series Cross-Validation with Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv_with_gaps(df, feature_columns, n_splits=5, gap=24):\n",
    "    \"\"\"\n",
    "    Time series cross-validation with gaps to prevent data leakage\n",
    "    Gap ensures no overlap between train and test periods\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    fold_size = n // (n_splits + 1)\n",
    "    results = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        # Define train and test indices with gap\n",
    "        train_end = fold_size * (i + 1)\n",
    "        test_start = train_end + gap  # Add gap\n",
    "        test_end = test_start + fold_size\n",
    "        \n",
    "        if test_end > n:\n",
    "            break\n",
    "        \n",
    "        train = df.iloc[:train_end]\n",
    "        test = df.iloc[test_start:test_end]\n",
    "        \n",
    "        X_train = train[feature_columns].values\n",
    "        y_train = train['target_vol_up'].values\n",
    "        X_test = test[feature_columns].values\n",
    "        y_test = test['target_vol_up'].values\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train best model type\n",
    "        if 'LightGBM' in best_model_name:\n",
    "            model = lgb.LGBMClassifier(n_estimators=100, max_depth=4, learning_rate=0.05,\n",
    "                                        subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "                                        class_weight='balanced', verbose=-1)\n",
    "        else:\n",
    "            model = xgb.XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.05,\n",
    "                                       subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "                                       use_label_encoder=False, eval_metric='logloss')\n",
    "        \n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        acc = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "        \n",
    "        results.append({\n",
    "            'fold': i+1,\n",
    "            'train': len(train),\n",
    "            'gap': gap,\n",
    "            'test': len(test),\n",
    "            'auc': auc,\n",
    "            'acc': acc\n",
    "        })\n",
    "        \n",
    "        print(f'Fold {i+1}: Train={len(train)}, Gap={gap}, Test={len(test)}, AUC={auc:.4f}, Acc={acc:.4f}')\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print('Time Series Cross-Validation with Gaps:')\n",
    "print('='*60)\n",
    "cv_results = time_series_cv_with_gaps(df_features, feature_columns, n_splits=4, gap=GAP_SIZE)\n",
    "print(f'\\nAverage AUC: {cv_results[\"auc\"].mean():.4f} ± {cv_results[\"auc\"].std():.4f}')\n",
    "print(f'Average Acc: {cv_results[\"acc\"].mean():.4f} ± {cv_results[\"acc\"].std():.4f}')\n",
    "print('\\n✓ Cross-validation complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Fixed Realistic Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realistic_backtest(test_df, probabilities, horizon=12, transaction_cost=0.0005, \n",
    "                       confidence_threshold=0.6, position_size=1.0):\n",
    "    \"\"\"\n",
    "    Realistic backtest with:\n",
    "    - Transaction costs per trade\n",
    "    - Confidence threshold filtering\n",
    "    - Position sizing based on prediction confidence\n",
    "    - Proper P&L calculation\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(test_df) - horizon):\n",
    "        prob = probabilities[i]\n",
    "        confidence = abs(prob - 0.5) * 2  # Convert to 0-1 scale\n",
    "        \n",
    "        # Only trade if confidence exceeds threshold\n",
    "        if confidence < confidence_threshold:\n",
    "            continue\n",
    "        \n",
    "        signal = 1 if prob > 0.5 else -1\n",
    "        current_vol = test_df.iloc[i]['rvol_12']\n",
    "        future_vol = test_df.iloc[i + horizon]['rvol_12']\n",
    "        \n",
    "        if pd.isna(future_vol) or pd.isna(current_vol) or current_vol == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate P&L\n",
    "        vol_change = (future_vol - current_vol) / current_vol\n",
    "        \n",
    "        # Scale position by confidence\n",
    "        size = position_size * confidence\n",
    "        \n",
    "        # P&L = signal * vol_change * position_size - transaction_costs\n",
    "        gross_pnl = signal * vol_change * size\n",
    "        net_pnl = gross_pnl - transaction_cost\n",
    "        \n",
    "        results.append({\n",
    "            'time': test_df.index[i],\n",
    "            'signal': signal,\n",
    "            'confidence': confidence,\n",
    "            'prediction': prob,\n",
    "            'position_size': size,\n",
    "            'vol_change': vol_change,\n",
    "            'gross_pnl': gross_pnl,\n",
    "            'transaction_cost': transaction_cost,\n",
    "            'net_pnl': net_pnl\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if len(results_df) > 0:\n",
    "        results_df['cum_pnl'] = results_df['net_pnl'].cumsum()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def calculate_backtest_metrics(results_df):\n",
    "    \"\"\"Calculate realistic backtest metrics\"\"\"\n",
    "    if len(results_df) == 0:\n",
    "        return {}\n",
    "    \n",
    "    pnl = results_df['net_pnl']\n",
    "    gross_pnl = results_df['gross_pnl']\n",
    "    \n",
    "    # Basic metrics\n",
    "    total_return = pnl.sum()\n",
    "    total_gross = gross_pnl.sum()\n",
    "    total_costs = results_df['transaction_cost'].sum()\n",
    "    win_rate = (pnl > 0).mean()\n",
    "    n_trades = len(pnl)\n",
    "    \n",
    "    # Sharpe ratio (annualized, assuming 252*288 5-min bars per year)\n",
    "    if pnl.std() > 0:\n",
    "        sharpe = pnl.mean() / pnl.std() * np.sqrt(252 * 288 / 12)  # Adjust for 12-bar horizon\n",
    "    else:\n",
    "        sharpe = 0\n",
    "    \n",
    "    # Max drawdown\n",
    "    cum_pnl = pnl.cumsum()\n",
    "    running_max = cum_pnl.cummax()\n",
    "    drawdown = cum_pnl - running_max\n",
    "    max_dd = drawdown.min()\n",
    "    \n",
    "    # Directional accuracy\n",
    "    direction_acc = (((results_df['signal'] > 0) & (results_df['vol_change'] > 0)) |\n",
    "                     ((results_df['signal'] < 0) & (results_df['vol_change'] < 0))).mean()\n",
    "    \n",
    "    # Average confidence\n",
    "    avg_confidence = results_df['confidence'].mean()\n",
    "    \n",
    "    return {\n",
    "        'total_return': total_return * 100,\n",
    "        'total_gross': total_gross * 100,\n",
    "        'total_costs': total_costs * 100,\n",
    "        'n_trades': n_trades,\n",
    "        'win_rate': win_rate,\n",
    "        'sharpe': sharpe,\n",
    "        'max_dd': max_dd * 100,\n",
    "        'direction_acc': direction_acc,\n",
    "        'avg_confidence': avg_confidence\n",
    "    }\n",
    "\n",
    "# Run realistic backtest\n",
    "print(f'Running realistic backtest with {best_model_name}...')\n",
    "print(f'Confidence threshold: {CONFIDENCE_THRESHOLD*100:.0f}%')\n",
    "\n",
    "# Use calibrated probabilities if available\n",
    "if best_model_name + ' (Calibrated)' in results:\n",
    "    probs_to_use = results[best_model_name + ' (Calibrated)']['probabilities']\n",
    "    model_name_used = best_model_name + ' (Calibrated)'\n",
    "else:\n",
    "    probs_to_use = results[best_model_name]['probabilities']\n",
    "    model_name_used = best_model_name\n",
    "\n",
    "backtest_results = realistic_backtest(\n",
    "    test_df,\n",
    "    probs_to_use,\n",
    "    horizon=HORIZON,\n",
    "    transaction_cost=TRANSACTION_COST,\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD\n",
    ")\n",
    "\n",
    "metrics = calculate_backtest_metrics(backtest_results)\n",
    "\n",
    "print(f'\\nRealistic Backtest Results ({model_name_used}):')\n",
    "print('='*60)\n",
    "print(f'Total Return (Net):     {metrics[\"total_return\"]:.2f}%')\n",
    "print(f'Total Return (Gross):   {metrics[\"total_gross\"]:.2f}%')\n",
    "print(f'Transaction Costs:      {metrics[\"total_costs\"]:.2f}%')\n",
    "print(f'Number of Trades:       {metrics[\"n_trades\"]}')\n",
    "print(f'Win Rate:               {metrics[\"win_rate\"]*100:.1f}%')\n",
    "print(f'Direction Accuracy:     {metrics[\"direction_acc\"]*100:.1f}%')\n",
    "print(f'Sharpe Ratio:           {metrics[\"sharpe\"]:.2f}')\n",
    "print(f'Max Drawdown:           {metrics[\"max_dd\"]:.2f}%')\n",
    "print(f'Avg Confidence:         {metrics[\"avg_confidence\"]*100:.1f}%')\n",
    "print('\\n✓ Backtest complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Regime-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by volatility regime\n",
    "def analyze_by_regime(test_df, backtest_results):\n",
    "    \"\"\"Analyze performance in different volatility regimes\"\"\"\n",
    "    \n",
    "    # Add regime information to backtest results\n",
    "    bt_with_regime = backtest_results.copy()\n",
    "    bt_with_regime['vol_level'] = test_df.loc[bt_with_regime['time'], 'rvol_24'].values\n",
    "    \n",
    "    # Define regimes based on percentiles\n",
    "    vol_25 = bt_with_regime['vol_level'].quantile(0.25)\n",
    "    vol_75 = bt_with_regime['vol_level'].quantile(0.75)\n",
    "    \n",
    "    bt_with_regime['regime'] = 'Medium'\n",
    "    bt_with_regime.loc[bt_with_regime['vol_level'] < vol_25, 'regime'] = 'Low Vol'\n",
    "    bt_with_regime.loc[bt_with_regime['vol_level'] > vol_75, 'regime'] = 'High Vol'\n",
    "    \n",
    "    # Calculate metrics by regime\n",
    "    regime_results = {}\n",
    "    \n",
    "    for regime in ['Low Vol', 'Medium', 'High Vol']:\n",
    "        regime_data = bt_with_regime[bt_with_regime['regime'] == regime]\n",
    "        \n",
    "        if len(regime_data) > 0:\n",
    "            pnl = regime_data['net_pnl']\n",
    "            regime_results[regime] = {\n",
    "                'n_trades': len(regime_data),\n",
    "                'total_return': pnl.sum() * 100,\n",
    "                'win_rate': (pnl > 0).mean(),\n",
    "                'direction_acc': (((regime_data['signal'] > 0) & (regime_data['vol_change'] > 0)) |\n",
    "                                  ((regime_data['signal'] < 0) & (regime_data['vol_change'] < 0))).mean(),\n",
    "                'avg_pnl': pnl.mean() * 100\n",
    "            }\n",
    "    \n",
    "    return regime_results, bt_with_regime\n",
    "\n",
    "if len(backtest_results) > 0:\n",
    "    regime_results, bt_with_regime = analyze_by_regime(test_df, backtest_results)\n",
    "    \n",
    "    print('\\nPerformance by Volatility Regime:')\n",
    "    print('='*70)\n",
    "    \n",
    "    for regime in ['Low Vol', 'Medium', 'High Vol']:\n",
    "        if regime in regime_results:\n",
    "            r = regime_results[regime]\n",
    "            print(f'\\n{regime}:')\n",
    "            print(f'  Trades:           {r[\"n_trades\"]}')\n",
    "            print(f'  Total Return:     {r[\"total_return\"]:.2f}%')\n",
    "            print(f'  Win Rate:         {r[\"win_rate\"]*100:.1f}%')\n",
    "            print(f'  Direction Acc:    {r[\"direction_acc\"]*100:.1f}%')\n",
    "            print(f'  Avg P&L/Trade:    {r[\"avg_pnl\"]:.4f}%')\n",
    "    \n",
    "    print('\\n✓ Regime analysis complete')\n",
    "else:\n",
    "    print('⚠ No trades in backtest - cannot perform regime analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. SHAP Value Analysis (Model Interpretability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    print('Computing SHAP values for model interpretability...')\n",
    "    print('(This may take a few minutes)')\n",
    "    \n",
    "    # Use a sample for SHAP (faster)\n",
    "    sample_size = min(1000, len(X_test_scaled))\n",
    "    X_sample = X_test_scaled[:sample_size]\n",
    "    \n",
    "    # Get the base model (before calibration)\n",
    "    base_model = trained_models[best_model_name]\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.TreeExplainer(base_model)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # If binary classification, select positive class\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_columns, show=False)\n",
    "    plt.title('SHAP Feature Importance', fontweight='bold', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n✓ SHAP analysis complete')\n",
    "else:\n",
    "    print('⚠ SHAP not available - skipping interpretability analysis')\n",
    "    print('  Install with: pip install shap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot backtest results\n",
    "if len(backtest_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Cumulative P&L\n",
    "    axes[0,0].plot(backtest_results['time'], backtest_results['cum_pnl'] * 100, linewidth=1.5)\n",
    "    axes[0,0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[0,0].set_title('Cumulative P&L (%)', fontweight='bold')\n",
    "    axes[0,0].set_xlabel('Time')\n",
    "    axes[0,0].set_ylabel('Cumulative Return (%)')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # P&L Distribution\n",
    "    axes[0,1].hist(backtest_results['net_pnl'] * 100, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0,1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "    axes[0,1].set_title('P&L Distribution', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('P&L per Trade (%)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Rolling Win Rate\n",
    "    window = min(50, len(backtest_results) // 4)\n",
    "    rolling_wr = (backtest_results['net_pnl'] > 0).rolling(window).mean() * 100\n",
    "    axes[1,0].plot(backtest_results['time'], rolling_wr, linewidth=1.5)\n",
    "    axes[1,0].axhline(y=50, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1,0].set_title(f'Rolling Win Rate ({window}-trade)', fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Time')\n",
    "    axes[1,0].set_ylabel('Win Rate (%)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence vs P&L\n",
    "    axes[1,1].scatter(backtest_results['confidence'], backtest_results['net_pnl'] * 100, \n",
    "                       alpha=0.4, s=20, c=backtest_results['net_pnl'], cmap='RdYlGn')\n",
    "    axes[1,1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[1,1].set_title('Prediction Confidence vs P&L', fontweight='bold')\n",
    "    axes[1,1].set_xlabel('Prediction Confidence')\n",
    "    axes[1,1].set_ylabel('P&L (%)')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('⚠ No trades to visualize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Enhanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced model with additional metadata\n",
    "model_data = {\n",
    "    'version': '2.0',\n",
    "    'scaler': scaler,\n",
    "    'models': trained_models,\n",
    "    'feature_columns': feature_columns,\n",
    "    'best_model': best_model_name,\n",
    "    'calibrated_model': calibrated_model,\n",
    "    'config': {\n",
    "        'symbol': SYMBOL,\n",
    "        'bar_freq': BAR_FREQ,\n",
    "        'horizon': HORIZON,\n",
    "        'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "        'transaction_cost': TRANSACTION_COST\n",
    "    },\n",
    "    'metrics': {\n",
    "        'test_auc': results[best_model_name]['auc'],\n",
    "        'cv_auc_mean': cv_results['auc'].mean(),\n",
    "        'cv_auc_std': cv_results['auc'].std(),\n",
    "        'backtest_metrics': metrics\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to Google Drive (persistent storage)\n",
    "save_path = '/content/drive/MyDrive/options-pricer/volatility_forecaster_v2_enhanced.joblib'\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "joblib.dump(model_data, save_path)\n",
    "print(f'✓ Enhanced model saved to Google Drive: {save_path}')\n",
    "\n",
    "# Also save a local copy for quick access\n",
    "joblib.dump(model_data, '/content/volatility_forecaster_v2_enhanced.joblib')\n",
    "print('✓ Local copy saved to /content/volatility_forecaster_v2_enhanced.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('              ENHANCED MODEL V2 - FINAL SUMMARY')\n",
    "print('='*70)\n",
    "print(f'''\n",
    "Data:\n",
    "  Symbol: {SYMBOL}\n",
    "  Bars: {len(bars):,}\n",
    "  Features: {len(feature_columns)} (including {len(new_features)} new features)\n",
    "\n",
    "Best Model: {best_model_name}\n",
    "  Test AUC:         {results[best_model_name]['auc']:.4f}\n",
    "  Test Accuracy:    {results[best_model_name]['accuracy']:.4f}\n",
    "  Edge vs Baseline: {(results[best_model_name]['accuracy'] - baseline_acc)*100:+.2f}%\n",
    "  Brier Score:      {results[best_model_name]['brier']:.4f}\n",
    "\n",
    "Cross-Validation (with {GAP_SIZE}-bar gaps):\n",
    "  AUC:              {cv_results['auc'].mean():.4f} ± {cv_results['auc'].std():.4f}\n",
    "  Accuracy:         {cv_results['acc'].mean():.4f} ± {cv_results['acc'].std():.4f}\n",
    "\n",
    "Realistic Backtest:\n",
    "  Net Return:       {metrics['total_return']:.2f}%\n",
    "  Gross Return:     {metrics['total_gross']:.2f}%\n",
    "  Costs:            {metrics['total_costs']:.2f}%\n",
    "  Trades:           {metrics['n_trades']}\n",
    "  Win Rate:         {metrics['win_rate']*100:.1f}%\n",
    "  Direction Acc:    {metrics['direction_acc']*100:.1f}%\n",
    "  Sharpe Ratio:     {metrics['sharpe']:.2f}\n",
    "  Max Drawdown:     {metrics['max_dd']:.2f}%\n",
    "\n",
    "Improvements Over V1:\n",
    "  ✓ Fixed unrealistic backtest\n",
    "  ✓ Added 6 advanced volatility features\n",
    "  ✓ Implemented probability calibration\n",
    "  ✓ Added time series CV with gaps\n",
    "  ✓ Included LightGBM model\n",
    "  ✓ Added SHAP interpretability\n",
    "  ✓ Implemented confidence filtering\n",
    "  ✓ Added regime-specific analysis\n",
    "''')\n",
    "\n",
    "print('='*70)\n",
    "print('✓ Enhanced volatility forecasting model complete!')\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}